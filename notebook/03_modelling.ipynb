{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.datacamp.com/es/tutorial/introduction-to-convolutional-neural-networks-cnns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants & Hyperparameters to define\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "NUM_WORDS = 5000\n",
    "MAX_SEQ_LEN = 50\n",
    "EMBEDDING_DIM = 50\n",
    "NUM_FILTERS = 64\n",
    "KERNEL_SIZE = 5\n",
    "NUM_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 12:00:59.548224: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-08 12:00:59.560464: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-08 12:00:59.578937: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-08 12:00:59.578973: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-08 12:00:59.590100: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-08 12:01:00.372605: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "\n",
    "# Import functions\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "# from support_model import\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape: (3599455, 5)\n",
      "train_data.shape: (35995, 5)\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "train_data = pd.read_csv('../data/train_data_preprocessed.csv')\n",
    "\n",
    "print(f'train_data.shape: {train_data.shape}')\n",
    "train_data = train_data.sample(frac=0.01)\n",
    "print(f'train_data.shape: {train_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequences:\n",
      "[[   3   68   31    1  318   12    3  160   50    1  207  116    2  235\n",
      "    66   18  178  161   39   66   15  180  594   78  664 4827 1191    1\n",
      "    64 1484    9 1962 4939   84   44  912  130    4  577  427   66   35\n",
      "    66    3  699  246 1711   11    8  410]\n",
      " [   8   13    1   30   74 1994 4062   20   12    3   19   53    2    3\n",
      "   259    6    6  346   42  263   70  116  290    4  603    3   33  327\n",
      "   157    6  163    8   20    9  566    2    3   37  133    6    4  198\n",
      "    71  961    4   53    0    0    0    0]\n",
      " [   1  301   35  132    1   46   28  265  189   11    5  467 2465  365\n",
      "     8    9   15    6  172   28   59 1936    4 1009  163    2   63  104\n",
      "    39    5   20    2 4465    5  167    7   54    2  837  108  161   12\n",
      "   311  375    1 1734    8    9   59   20]\n",
      " [   8    9   95   70  915    2   95   70  246    3   19  306   27  174\n",
      "   226   38   14   19    5    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]\n",
      " [   3  123    8   96   11  417    2    6 1611  390   21   96  423   10\n",
      "    21  621   49   24    7    1  103  595    7   27   54  234  218    9\n",
      "   122  205 2439    2    6 3326    4   59  596    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "text_data = train_data['text']\n",
    "\n",
    "# Text preprocessing\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS)\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "\n",
    "# Padding sequences\n",
    "sequences = pad_sequences(\n",
    "    sequences, \n",
    "    maxlen=MAX_SEQ_LEN, \n",
    "    padding='post')\n",
    "\n",
    "print(f'sequences:\\n{sequences[:5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels:\n",
      "         _label_0  _label_1\n",
      "157027          0         1\n",
      "606029          0         1\n",
      "1821717         1         0\n",
      "1396672         0         1\n",
      "2255680         0         1\n",
      "\n",
      "labels_encoded:\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "labels = pd.get_dummies(train_data['labels'], prefix='_label', dtype=int)\n",
    "\n",
    "print(f'labels:\\n{labels[:5]}')\n",
    "\n",
    "# One-hot encode labels\n",
    "labels_encoded = to_categorical(\n",
    "    labels['_label_0'], \n",
    "    num_classes=NUM_CLASSES)\n",
    "\n",
    "print(f'\\nlabels_encoded:\\n{labels_encoded[:5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (28796, 50)\n",
      "y_train shape: (28796, 2)\n"
     ]
    }
   ],
   "source": [
    "# Split data into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    sequences, \n",
    "    labels_encoded, \n",
    "    test_size=0.2, \n",
    "    random_state=RANDOM_SEED)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "**Sequential convolutional neural network (CNN) for text classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Embedding Layer\n",
    "* `Embedding(input_dim=5000, output_dim=100, input_length=100)`\n",
    "* `input_dim=5000`: This specifies the vocabulary size, meaning the model can handle up to **5000** unique words.\n",
    "* `output_dim=100`: This defines the dimensionality of the embedding vector, which compresses each word into a **100**-dimensional vector.\n",
    "* `input_length=100`: This sets the maximum length of the input text sequences (sentences or paragraphs) to **100** words.\n",
    "\n",
    "2. Convolutional Layer\n",
    "* `Conv1D(filters=64, kernel_size=5, activation='relu')`: This 1D convolutional layer extracts features from the embedded text sequences.\n",
    "* `filters=64`: This indicates the number of filters used to identify patterns in the text.\n",
    "* `kernel_size=5`: This defines the size of the window that the filter slides over the text sequence (**5** words in this case).\n",
    "* `activation='relu'`: This activation function introduces non-linearity, allowing the model to learn complex relationships between words.\n",
    "\n",
    "    * `'relu'` means Rectified Linear Unit (ReLU). \n",
    "    * For any input value $(x)$, it outputs the value itself if it's positive $(x > 0)$ and zero otherwise $(x <= 0)$. \n",
    "    * Mathematically, it can be represented as:\n",
    "    * $f(x) = max(0, x)$\n",
    "\n",
    "3. Pooling Layer\n",
    "* `MaxPooling1D(pool_size=4)`: This layer reduces the dimensionality of the data by taking the maximum value from every window of size **4** along the sequence This helps control overfitting and focuses on the most important features.\n",
    "\n",
    "4. Flattening Layer\n",
    "* `Flatten()`: This layer transforms the 2D output from the convolutional layer into a 1D vector suitable for feeding into the fully connected layers.\n",
    "\n",
    "5. Fully Connected Layers\n",
    "* `Dense(10, activation='relu')`: This first fully connected layer has **10** neurons and uses the ReLU activation function. It learns higher-level features by combining the extracted features from the convolutional layers.\n",
    "\n",
    "* `Dense(3, activation='softmax')`: This final fully connected layer has 3 neurons and uses the softmax activation function. It outputs a probability distribution over 3 categories, making it suitable for multi-class classification tasks (e.g., classifying text into 3 different genres).\n",
    "\n",
    "    * `'softmax'`: For each element $(i)$ in the input vector, softmax calculates the probability $(p_i)$ using the following formula:\n",
    "    * $p_i = exp(x_i) / Σ(exp(x_j))$  for all $j$ in the vector\n",
    "    * Here, $exp(x_i)$ represents the exponentiation of the i-th element in the input vector.\n",
    "    * $Σ(exp(x_j))$ represents the sum of the exponentials of all elements in the vector.\n",
    "\n",
    "6. Compiling the Model:\n",
    "* `model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])`: This compiles the model by specifying the optimizer (Adaptive Moment Estimation (Adam) for efficient training), the loss function (sparse categorical crossentropy for multi-class classification), and the metrics (accuracy to measure performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 12:01:16.173108: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-08 12:01:16.212353: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-08 12:01:16.212541: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-08 12:01:16.213685: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-08 12:01:16.213795: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-08 12:01:16.213859: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-08 12:01:16.266402: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-08 12:01:16.266542: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-08 12:01:16.266617: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-07-08 12:01:16.266677: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6241 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Argument `output` must have rank (ndim) `target.ndim - 1`. Received: target.shape=(None, 2), output.shape=(None, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 26\u001b[0m\n\u001b[1;32m     21\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m     22\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     23\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/env_001/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/env_001/lib/python3.9/site-packages/keras/src/backend/tensorflow/nn.py:642\u001b[0m, in \u001b[0;36msparse_categorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `output` must be at least rank 1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    639\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    640\u001b[0m     )\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m--> 642\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    643\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `output` must have rank (ndim) `target.ndim - 1`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    645\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    646\u001b[0m     )\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n",
      "\u001b[0;31mValueError\u001b[0m: Argument `output` must have rank (ndim) `target.ndim - 1`. Received: target.shape=(None, 2), output.shape=(None, 1)"
     ]
    }
   ],
   "source": [
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "    Embedding(\n",
    "        input_dim=MAX_SEQ_LEN, \n",
    "        output_dim=EMBEDDING_DIM, \n",
    "        input_length=MAX_SEQ_LEN),\n",
    "    Conv1D(\n",
    "        filters=NUM_FILTERS, \n",
    "        kernel_size=KERNEL_SIZE, \n",
    "        activation='relu', \n",
    "        padding='same'),\n",
    "    MaxPooling1D(\n",
    "        pool_size=4, \n",
    "        padding='same'),\n",
    "    Flatten(),\n",
    "    Dense(1, activation='relu'),\n",
    "    Dense(NUM_CLASSES, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', \n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy', 'precision', 'recall'])\n",
    "\n",
    "# Train model\n",
    "model.fit(x=X_train, \n",
    "    y=y_train, \n",
    "    epochs=10, \n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_loss, model_accuracy, model_precision, model_recall, model_categorical_crossentropy, model_auc, model_f1_score = model.evaluate(X_test, y_test)\n",
    "# print(\"F1 Score:\", model_f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
